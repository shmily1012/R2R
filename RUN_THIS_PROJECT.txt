R2R Quick Start (Linux, with Separate Generation and Embedding Bases)

1) Prerequisites
- Python 3.12
- Postgres database (connection via env vars)
- API endpoints:
  - Text generation (OpenAI-compatible): MODEL_BASE_URL
  - Embedding (OpenAI-compatible): EMBEDDING_BASE_URL

2) Install (Linux)
- Create and activate a virtualenv, then install from ./py
  - python -m venv .venv
  - source .venv/bin/activate
  - pip install -e ./py

3) Environment Variables (Linux shell)
- Generation (OpenAI-compatible)
  - export LMSTUDIO_API_BASE=http://cerebrogen.skhms.com/api   # or .../api/v1
  - export LMSTUDIO_API_KEY="<your api token>"
  - Note: r2r.toml uses model lmstudio/llm

- Embedding (separate OpenAI-compatible base)
  - export OPENAI_API_BASE=http://172.28.139.49/rse/embedding/v1
  - export OPENAI_API_KEY="<your embedding api token>"
  - r2r.toml uses openai/gte-Qwen2-1.5B-instruct with dimension 1536

- Postgres (example)
  - export R2R_POSTGRES_USER=your_user
  - export R2R_POSTGRES_PASSWORD=your_password
  - export R2R_POSTGRES_HOST=your_host
  - export R2R_POSTGRES_PORT=5432
  - export R2R_POSTGRES_DBNAME=your_db
  - export R2R_PROJECT_NAME=your_project_name

4) Start the API server (local)
- R2R_PORT=8002 python -m r2r.serve
- Serves on http://0.0.0.0:8002

- 5) Verify (basic checks)
- OpenAPI spec (local): http://localhost:8002/openapi_spec
- OpenAPI spec (Docker dev): http://localhost:8002/openapi_spec

- Completion (text generation)
  - Local:  curl -X POST http://localhost:8002/v3/retrieval/completion \
              -H "Content-Type: application/json" \
              -d '{"messages":[{"role":"user","content":"Say hello in Chinese."}],"generation_config":{"model":"lmstudio/llm","stream":false, "max_tokens":100}}'
  - Docker: curl -X POST http://localhost:8002/v3/retrieval/completion -H "Content-Type: application/json" -d '{"messages":[{"role":"user","content":"Say hello in Chinese."}],"generation_config":{"model":"lmstudio/llm","stream":false, "max_tokens":100}}'

- Embedding
  - Local:  curl -X POST http://localhost:8002/v3/retrieval/embedding \
              -H "Content-Type: application/json" \
              -d '{"text":"What is DeepSeek R1?"}'
  - Docker: curl -X POST http://localhost:8002/v3/retrieval/embedding \
              -H "Content-Type: application/json" \
              -d '{"text":"What is DeepSeek R1?"}'

6) Docker Dev (Linux)
- Compose file: docker/compose.dev.yaml (runs Postgres + R2R on port 8002)
- Option A: Inline env in the compose (edit services.r2r.environment)
- Option B: Put env in docker/env/r2r.env and uncomment env_file in the compose
- Start:
  - cd docker
  - docker compose -f compose.dev.yaml up -d --build
- Logs: docker compose -f compose.dev.yaml logs -f
- Verify: http://localhost:8002/openapi_spec

7) How this project routes models
- Text generation uses the OpenAICompletionProvider with model prefix lmstudio/...
  - Base URL from LMSTUDIO_API_BASE
- Embeddings use LiteLLM with model openai/gte-Qwen2-1.5B-instruct
  - Base URL from OPENAI_API_BASE
- r2r.toml pins both embedding configs to dimension 1536; both must match

8) Common Issues
- 404/405 on generation: ensure LMSTUDIO_API_BASE has the correct path (try adding /v1)
- Authentication errors: verify LMSTUDIO_API_KEY and OPENAI_API_KEY
- Dimension mismatch: keep [embedding] and [completion_embedding] base_dimension identical (1536)
- Database connection: ensure all R2R_POSTGRES_* variables are correct and the DB is reachable

9) Useful Files
- Main config: py/r2r/r2r.toml
- Example env template: .env.example
- Server entrypoint: py/r2r/serve.py
- REST endpoints: py/core/main/api/v3

10) Next Steps
- Ingest documents via /v3/documents endpoints, then try /v3/retrieval/rag for RAG answers
- Tune generation defaults in [completion.generation_config] inside py/r2r/r2r.toml
