{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2R RAG Quickstart (Linux)\n",
    "\n",
    "This notebook shows how to use R2R for Retrieval-Augmented Generation (RAG):\n",
    "- Install the Python SDK\n",
    "- Verify the API is reachable\n",
    "- Ingest a sample document\n",
    "- Run search and RAG\n",
    "- Try the agent for deeper analysis\n",
    "\n",
    "To run, start the R2R API separately in a Linux shell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801aacf9",
   "metadata": {},
   "source": [
    "## Prerequisites (run in a separate terminal)\n",
    "\n",
    "1) Create a Python 3.12 venv and install the server (from this repo):\n",
    "```bash\n",
    "python3.12 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -e ./py\n",
    "```\n",
    "\n",
    "2) Configure your providers (OpenAI-compatible):\n",
    "These examples mirror defaults in docker/compose.dev.yaml.\n",
    "```bash\n",
    "# Text generation (e.g., LM Studio / OpenAI-compatible)\n",
    "export LMSTUDIO_API_BASE=\"http://localhost:8000/v1\"  # from docker/compose.dev.yaml\n",
    "export LMSTUDIO_API_KEY=\"123\"  # example key used in compose.dev.yaml; replace as needed\n",
    "\n",
    "# Embeddings (can be a different OpenAI-compatible base)\n",
    "export OPENAI_API_BASE=\"http://athena.skhms.com/rse/embedding/qwen3-4b/v1\"\n",
    "export OPENAI_API_KEY=\"<embed-token>\"  # set your key; compose.dev.yaml shows a sample\n",
    "\n",
    "# Postgres (adjust as needed)\n",
    "export R2R_POSTGRES_HOST=127.0.0.1  # from compose.dev.yaml\n",
    "export R2R_POSTGRES_PORT=5432\n",
    "export R2R_POSTGRES_USER=r2r\n",
    "export R2R_POSTGRES_PASSWORD=r2rpassword\n",
    "export R2R_POSTGRES_DBNAME=r2r\n",
    "export R2R_PROJECT_NAME=r2r_local\n",
    "```\n",
    "\n",
    "Note: Ensure embedding dimensions in config match (see py/r2r/r2r.toml).\n",
    "\n",
    "3) Start the API server:\n",
    "```bash\n",
    "export R2R_PORT=8002\n",
    "python -m r2r.serve\n",
    "# Server listens on http://0.0.0.0:8002\n",
    "```\n",
    "\n",
    "Then continue here in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c9c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the Python SDK for the client\n",
    "%pip -q install r2r requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac18660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured:\n",
      " - LMSTUDIO_API_BASE=http://localhost:8000/v1\n",
      " - LMSTUDIO_API_KEY=123\n",
      " - OPENAI_API_BASE=http://athena.skhms.com/rse/embedding/qwen3-4b/v1\n",
      " - R2R_POSTGRES_HOST=127.0.0.1\n",
      " - R2R_POSTGRES_PORT=5432\n",
      " - R2R_POSTGRES_DBNAME=r2r\n",
      " - R2R_PROJECT_NAME=r2r_local\n"
     ]
    }
   ],
   "source": [
    "# Configure providers and Postgres via Python env (this kernel only)\n",
    "# If your R2R server runs in a separate process,\n",
    "# set these in that process as well (compose/env).\n",
    "import os\n",
    "\n",
    "def setdefault_env(key, value):\n",
    "    # if not os.environ.get(key):\n",
    "    os.environ[key] = str(value)\n",
    "\n",
    "# LM Studio / text generation (compose.dev.yaml defaults)\n",
    "setdefault_env(\"LMSTUDIO_API_BASE\", \"http://localhost:8000/v1\")\n",
    "setdefault_env(\"LMSTUDIO_API_KEY\", \"123\")  # example; replace with your token\n",
    "\n",
    "# OpenAI-compatible embeddings\n",
    "setdefault_env(\"OPENAI_API_BASE\", \"http://athena.skhms.com/rse/embedding/qwen3-4b/v1\")\n",
    "setdefault_env(\"OPENAI_API_KEY\", \"rse-s0HBWeEhWUXJx5LI3gNNtJWHdKBOePvRVuh8s3BplYRiT7VZ0N2aMej\")\n",
    "\n",
    "\n",
    "# Postgres\n",
    "setdefault_env(\"R2R_POSTGRES_HOST\", \"127.0.0.1\")\n",
    "setdefault_env(\"R2R_POSTGRES_PORT\", \"5432\")\n",
    "setdefault_env(\"R2R_POSTGRES_USER\", \"r2r\")\n",
    "setdefault_env(\"R2R_POSTGRES_PASSWORD\", \"r2rpassword\")\n",
    "setdefault_env(\"R2R_POSTGRES_DBNAME\", \"r2r\")\n",
    "setdefault_env(\"R2R_PROJECT_NAME\", \"r2r_local\")\n",
    "\n",
    "print(\"Configured:\")\n",
    "for k in [\n",
    "    \"LMSTUDIO_API_BASE\",\"LMSTUDIO_API_KEY\",\n",
    "    \"OPENAI_API_BASE\",\n",
    "    \"R2R_POSTGRES_HOST\",\"R2R_POSTGRES_PORT\",\"R2R_POSTGRES_DBNAME\",\"R2R_PROJECT_NAME\"\n",
    "]:\n",
    "    print(f\" - {k}={os.environ.get(k)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b15d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BASE_URL = http://0.0.0.0:8002\n"
     ]
    }
   ],
   "source": [
    "# Point to your running R2R server\n",
    "import os\n",
    "BASE_URL = os.getenv('R2R_BASE_URL', 'http://0.0.0.0:8002')\n",
    "print('Using BASE_URL =', BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4febd60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAPI available at http://0.0.0.0:8002/openapi_spec\n",
      "Title: R2R Application API\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: hit the OpenAPI spec\n",
    "import requests, json\n",
    "spec_url = BASE_URL + '/openapi_spec'\n",
    "try:\n",
    "    r = requests.get(spec_url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    print('OpenAPI available at', spec_url)\n",
    "    # print a small part of the spec\n",
    "    data = r.json() if r.headers.get('content-type','').startswith('application/json') else r.text\n",
    "    if isinstance(data, dict):\n",
    "        print('Title:', data.get('info', {}).get('title'))\n",
    "    else:\n",
    "        print(str(data)[:200] + '...')\n",
    "except Exception as e:\n",
    "    print('Failed to reach server:', e)\n",
    "    print('Make sure the server is running as described above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db0b0f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sdk.sync_client.R2RClient at 0x7fd8667b1970>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the client\n",
    "from r2r import R2RClient\n",
    "client = R2RClient(base_url=BASE_URL)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772bea40",
   "metadata": {},
   "source": [
    "## Ingest a sample document\n",
    "\n",
    "You can upload your own `.pdf`, `.txt`, `.md`, etc.\n",
    "Below we create a small `.txt` file and ingest it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b8aad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "R2RException",
     "evalue": "An error '500: Error during ingestion: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 133475 tokens (13475 in the messages, 120000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}' occurred during create_document",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mR2RException\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m sample_path = \u001b[33m'\u001b[39m\u001b[33mNCB-PCI_Express_Base_6.3.txt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# with open(sample_path, 'w', encoding='utf-8') as f:\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#     f.write('DeepSeek R1 is a reasoning model. This file is a simple demo.\\n')\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#     f.write('RAG combines retrieval with generation to produce grounded answers.')\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# print('Wrote', sample_path)\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Ingest the file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m doc = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/nvme1/workspace/R2R/py/sdk/sync_methods/documents.py:120\u001b[39m, in \u001b[36mDocumentsSDK.create\u001b[39m\u001b[34m(self, file_path, raw_text, chunks, s3_url, id, ingestion_mode, collection_ids, metadata, ingestion_config, run_with_orchestration)\u001b[39m\n\u001b[32m    113\u001b[39m files = [\n\u001b[32m    114\u001b[39m     (\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    116\u001b[39m         (filename, file_instance, \u001b[33m\"\u001b[39m\u001b[33mapplication/octet-stream\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    117\u001b[39m     )\n\u001b[32m    118\u001b[39m ]\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mv3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Ensure we close the file after the request is complete\u001b[39;00m\n\u001b[32m    129\u001b[39m     file_instance.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/nvme1/workspace/R2R/py/sdk/sync_client.py:52\u001b[39m, in \u001b[36mR2RClient._make_request\u001b[39m\u001b[34m(self, method, endpoint, version, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     51\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.client.request(method, url, **request_args)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     55\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response.json() \u001b[38;5;28;01mif\u001b[39;00m response.content \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/nvme1/workspace/R2R/py/sdk/sync_client.py:145\u001b[39m, in \u001b[36mR2RClient._handle_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    143\u001b[39m     message = \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m R2RException(\n\u001b[32m    146\u001b[39m     status_code=response.status_code, message=message\n\u001b[32m    147\u001b[39m )\n",
      "\u001b[31mR2RException\u001b[39m: An error '500: Error during ingestion: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 131072 tokens. However, you requested 133475 tokens (13475 in the messages, 120000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}' occurred during create_document"
     ]
    }
   ],
   "source": [
    "# Create a small sample file\n",
    "sample_path = 'NCB-PCI_Express_Base_6.3.txt'\n",
    "# with open(sample_path, 'w', encoding='utf-8') as f:\n",
    "#     f.write('DeepSeek R1 is a reasoning model. This file is a simple demo.\\n')\n",
    "#     f.write('RAG combines retrieval with generation to produce grounded answers.')\n",
    "# print('Wrote', sample_path)\n",
    "\n",
    "# Ingest the file\n",
    "doc = client.documents.create(file_path=sample_path)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af258041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentResponse(id=UUID('40030f07-db03-59c3-ba46-7376251cd7b1'), collection_ids=[UUID('122fdf6a-e116-546b-a8f6-e4cb2e2c0a09')], owner_id=UUID('2acb499e-8428-543b-bd85-0d9098718220'), document_type=<DocumentType.TXT: 'txt'>, metadata={'version': 'v0'}, title='sample_doc.txt', version='v0', size_in_bytes=129, ingestion_status=<IngestionStatus.AUGMENTING: 'augmenting'>, extraction_status=<GraphExtractionStatus.PENDING: 'pending'>, created_at=datetime.datetime(2025, 9, 5, 18, 22, 24, 212274, tzinfo=TzInfo(UTC)), updated_at=datetime.datetime(2025, 9, 5, 18, 22, 24, 218906, tzinfo=TzInfo(UTC)), ingestion_attempt_number=None, summary=None, summary_embedding=None, total_tokens=27, chunks=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List your documents\n",
    "docs = client.documents.list()\n",
    "len(docs)\n",
    "docs[0] if docs else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92035978",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Run a semantic or hybrid search over ingested content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdaa8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2RResults[AggregateSearchResult](results=AggregateSearchResult(chunk_search_results=[ChunkSearchResult(score=0.660, text=DeepSeek R1 is a reasoning model. This file is a simple demo.\n",
       "RAG combines retrieval with generation to produce grounded answers.)], graph_search_results=[], web_page_search_results=None, web_search_results=None, document_search_results=None, generic_tool_result=None))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_res = client.retrieval.search(query='What is RAG?')\n",
    "search_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1dff30",
   "metadata": {},
   "source": [
    "## RAG (with citations)\n",
    "\n",
    "Ask a question and let R2R retrieve + generate an answer grounded in your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402c797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2RResults[RAGResponse](results=RAGResponse(generated_answer='RAG (Retrieval-Augmented Generation) is a technique that combines retrieval from a knowledge source with a generation model to produce more accurate and grounded answers [e921491].', search_results=AggregateSearchResult(chunk_search_results=[ChunkSearchResult(score=0.827, text=DeepSeek R1 is a reasoning model. This file is a simple demo.\n",
       "RAG combines retrieval with generation to produce grounded answers.)], graph_search_results=[], web_page_search_results=None, web_search_results=None, document_search_results=None, generic_tool_result=None), citations=[Citation(id='e921491', object='citation', is_new=True, span=None, source_type=None, payload={'id': 'e921491c-3bd6-57f7-ae96-fa615ecb8f99', 'document_id': '40030f07-db03-59c3-ba46-7376251cd7b1', 'owner_id': '2acb499e-8428-543b-bd85-0d9098718220', 'collection_ids': ['122fdf6a-e116-546b-a8f6-e4cb2e2c0a09'], 'score': 0.8274951657302877, 'text': 'DeepSeek R1 is a reasoning model. This file is a simple demo.\\nRAG combines retrieval with generation to produce grounded answers.', 'metadata': {'version': 'v0', 'chunk_order': 0, 'document_type': 'txt', 'associated_query': 'Explain RAG briefly.'}})], metadata={'id': 'chatcmpl-4e4afecbcf404549b7befc394c91523e', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': [], 'structured_content': None}}], 'created': 1757098815, 'model': 'llm', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': None, 'usage': {'completion_tokens': 41, 'prompt_tokens': 171, 'total_tokens': 212, 'completion_tokens_details': None, 'prompt_tokens_details': None}}, completion='RAG (Retrieval-Augmented Generation) is a technique that combines retrieval from a knowledge source with a generation model to produce more accurate and grounded answers [e921491].'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_res = client.retrieval.rag(query='Explain RAG briefly.')\n",
    "rag_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic RAG\n",
    "\n",
    "Use the conversational agent with retrieval tools for richer, multi-step answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "R2RException",
     "evalue": "{'message': '500: Internal Server Error - Error code: 400 - {\\'object\\': \\'error\\', \\'message\\': \\'\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set\\', \\'type\\': \\'BadRequestError\\', \\'param\\': None, \\'code\\': 400}', 'error_type': 'R2RException'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mR2RException\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m agent_res = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieval\u001b[49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGive a short analysis of RAG.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrag_tools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_file_knowledge\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget_file_content\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m agent_res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/nvme1/workspace/R2R/py/sdk/sync_methods/retrieval.py:462\u001b[39m, in \u001b[36mRetrievalSDK.agent\u001b[39m\u001b[34m(self, message, rag_generation_config, research_generation_config, search_mode, search_settings, task_prompt, include_title_if_available, conversation_id, max_tool_context_length, use_system_context, rag_tools, research_tools, tools, mode, needs_initial_conversation_name)\u001b[39m\n\u001b[32m    454\u001b[39m     raw_stream = \u001b[38;5;28mself\u001b[39m.client._make_streaming_request(\n\u001b[32m    455\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    456\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mretrieval/agent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    457\u001b[39m         json=data,\n\u001b[32m    458\u001b[39m         version=\u001b[33m\"\u001b[39m\u001b[33mv3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    459\u001b[39m     )\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (parse_retrieval_event(event) \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m raw_stream)\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieval/agent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mv3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m WrappedAgentResponse(**response_dict)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/nvme1/workspace/R2R/py/sdk/sync_client.py:52\u001b[39m, in \u001b[36mR2RClient._make_request\u001b[39m\u001b[34m(self, method, endpoint, version, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     51\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.client.request(method, url, **request_args)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     55\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response.json() \u001b[38;5;28;01mif\u001b[39;00m response.content \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/nvme1/workspace/R2R/py/sdk/sync_client.py:145\u001b[39m, in \u001b[36mR2RClient._handle_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    143\u001b[39m     message = \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m R2RException(\n\u001b[32m    146\u001b[39m     status_code=response.status_code, message=message\n\u001b[32m    147\u001b[39m )\n",
      "\u001b[31mR2RException\u001b[39m: {'message': '500: Internal Server Error - Error code: 400 - {\\'object\\': \\'error\\', \\'message\\': \\'\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set\\', \\'type\\': \\'BadRequestError\\', \\'param\\': None, \\'code\\': 400}', 'error_type': 'R2RException'}"
     ]
    }
   ],
   "source": [
    "agent_res = client.retrieval.agent(\n",
    "    message={\"role\": \"user\", \"content\": \"Give a short analysis of RAG.\"},\n",
    "    rag_tools=[\"search_file_knowledge\", \"get_file_content\"],\n",
    ")\n",
    "agent_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: custom search settings\n",
    "\n",
    "You can pass `search_mode` and `search_settings` to control hybrid search, filters, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2RResults[RAGResponse](results=RAGResponse(generated_answer='The sample states that RAG combines retrieval with generation to produce grounded answers [e921491].', search_results=AggregateSearchResult(chunk_search_results=[ChunkSearchResult(score=0.018, text=DeepSeek R1 is a reasoning model. This file is a simple demo.\n",
       "RAG combines retrieval with generation to produce grounded answers.), ChunkSearchResult(score=0.018, text=DeepSeek R1 is a reasoning model. This file is a simple demo.\n",
       "RAG combines retrieval with generation to produce grounded answers.), ChunkSearchResult(score=0.018, text=DeepSeek R1 is a reasoning model. This file is a simple demo.\n",
       "RAG combines retrieval with generation to produce grounded answers.), ChunkSearchResult(score=0.018, text=DeepSeek R1 is a reasoning model. This file is a simple demo.\n",
       "RAG combines retrieval with generation to produce grounded answers.)], graph_search_results=[], web_page_search_results=None, web_search_results=None, document_search_results=None, generic_tool_result=None), citations=[Citation(id='e921491', object='citation', is_new=True, span=None, source_type=None, payload={'id': 'e921491c-3bd6-57f7-ae96-fa615ecb8f99', 'document_id': '40030f07-db03-59c3-ba46-7376251cd7b1', 'owner_id': '2acb499e-8428-543b-bd85-0d9098718220', 'collection_ids': ['122fdf6a-e116-546b-a8f6-e4cb2e2c0a09'], 'score': 0.018006535947712416, 'text': 'DeepSeek R1 is a reasoning model. This file is a simple demo.\\nRAG combines retrieval with generation to produce grounded answers.', 'metadata': {'version': 'v0', 'chunk_order': 0, 'document_type': 'txt', 'semantic_rank': 1, 'full_text_rank': 50, 'associated_query': 'What does the sample say about RAG?'}})], metadata={'id': 'chatcmpl-527351a9d8114ce7aabc6ce47f78ee39', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': [], 'structured_content': None}}], 'created': 1757098889, 'model': 'llm', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': None, 'usage': {'completion_tokens': 24, 'prompt_tokens': 291, 'total_tokens': 315, 'completion_tokens_details': None, 'prompt_tokens_details': None}}, completion='The sample states that RAG combines retrieval with generation to produce grounded answers [e921491].'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advanced = client.retrieval.rag(\n",
    "    query='What does the sample say about RAG?',\n",
    "    search_mode='advanced',\n",
    "    search_settings={\n",
    "        \"use_hybrid_search\": True,\n",
    "        \"hybrid_settings\": {\n",
    "            \"full_text_weight\": 1.0,\n",
    "            \"semantic_weight\": 5.0,\n",
    "            \"full_text_limit\": 50,\n",
    "            \"rrf_k\": 50\n",
    "        }\n",
    "    }\n",
    ")\n",
    "advanced"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
