[app]
# app settings are global available like `r2r_config.agent.app`
# project_name = "r2r_default" # optional, can also set with `R2R_PROJECT_NAME` env var
default_max_documents_per_user = 10_000
default_max_chunks_per_user = 10_000_000
default_max_collections_per_user = 5_000

# Set the default max upload size to 200 GB for local testing
default_max_upload_size = 214748364800

# LLM used for internal operations, like deriving conversation names
# Prefer LM Studio locally; model name must exist on your LM Studio server.
# With prefix "lmstudio/", the base URL is taken from LMSTUDIO_API_BASE.
fast_llm = "lmstudio/llm"

# LLM used for user-facing output, like RAG replies
# Using local LLM server
quality_llm = "lmstudio/llm"

# LLM used for ingesting visual inputs
vlm = "lmstudio/llm"

# LLM used for transcription
audio_lm = "openai/whisper-1"

# Reasoning model, used for `research` agent
reasoning_llm = "lmstudio/llm"
# Planning model, used for `research` agent
planning_llm = "lmstudio/llm"


[agent]
rag_agent_static_prompt = "static_rag_agent"
rag_agent_dynamic_prompt = "dynamic_rag_agent"
# The following tools are available to the `rag` agent
rag_tools = ["search_file_descriptions", "search_file_knowledge", "get_file_content"] # can add  "web_search" | "web_scrape"
# The following tools are available to the `research` agent
research_tools = ["rag", "reasoning", "critique", "python_executor"]

[auth]
provider = "r2r"
access_token_lifetime_in_minutes = 60000
refresh_token_lifetime_in_days = 7
require_authentication = false
require_email_verification = false
default_admin_email = "admin@example.com"
default_admin_password = "change_me_immediately"

[completion]
# Use the OpenAICompletionProvider which supports OpenAI-like servers by model prefix.
# With model prefix "lmstudio/", the base URL is taken from LMSTUDIO_API_BASE and
# API key from LMSTUDIO_API_KEY.
provider = "openai"
concurrent_request_limit = 64
request_timeout = 60

  [completion.generation_config]
  # Defaults for your TEXT model
  temperature = 0.7
  top_p = 0.8
  max_tokens_to_sample = 131072
  stream = false
  # Optional extra args your server might accept (not all providers use these)
  add_generation_kwargs = { top_k = 20, repetition_penalty = 1.05, presence_penalty = 1.5 }

[crypto]
provider = "bcrypt"

[file]
provider = "postgres"

[database]
# Database connection settings
user = "r2r"
password = "r2rpassword"
host = "localhost"
port = 5432
db_name = "r2r"

default_collection_name = "Default"
default_collection_description = "Your default collection."
collection_summary_prompt = "collection_summary"

  [database.graph_creation_settings]
    graph_entity_description_prompt = "graph_entity_description"
    graph_extraction_prompt = "graph_extraction"
    entity_types = [] # if empty, all entities are extracted
    relation_types = [] # if empty, all relations are extracted
    automatic_deduplication = true # enable automatic deduplication of entities

  [database.graph_enrichment_settings]
    graph_communities_prompt = "graph_communities"

  [database.maintenance]
    vacuum_schedule = "0 3 * * *"  # Run at 3:00 AM daily

[embedding]
# Use LiteLLM for embeddings so we can point to a separate OpenAI-like base URL.
# Set OPENAI_API_BASE to your embedding server, e.g.:
OPENAI_API_BASE="http://athena.skhms.com/rse/embedding/qwen3-4b/v1/"
OPENAI_API_KEY="rse-s0HBWeEhWUXJx5LI3gNNtJWHdKBOePvRVuh8s3BplYRiT7VZ0N2aMej"
provider = "litellm"
base_model = "Qwen-Qwen3-Embedding-4B"
base_dimension = 1536
# rerank_model = "huggingface/mixedbread-ai/mxbai-rerank-large-v1" # optional reranking model
batch_size = 128
concurrent_request_limit = 256
initial_backoff = 1.0
quantization_settings = { quantization_type = "FP32" }
litellm_drop_params = true

# LiteLLM global settings
[litellm]
drop_params = true

[completion_embedding]
# Keep this consistent with [embedding] so vector dimensions match
provider = "litellm"
base_model = "Qwen-Qwen3-Embedding-4B"
base_dimension = 1536
batch_size = 128
concurrent_request_limit = 256

[ingestion]
provider = "r2r"
chunking_strategy = "recursive"
chunk_size = 1_024
chunk_overlap = 512
excluded_parsers = []
automatic_extraction = true # enable automatic extraction of entities and relations
vlm_batch_size=20
max_concurrent_vlm_tasks=20
vlm_ocr_one_page_per_chunk = true

# Use LM Studio for document summaries during ingestion
document_summary_model = "lmstudio/llama-3.2-3b-instruct"

  [ingestion.chunk_enrichment_settings]
    chunk_enrichment_prompt = "chunk_enrichment"
    enable_chunk_enrichment = false # disabled by default
    n_chunks = 2 # the number of chunks (both preceding and succeeding) to use in enrichment

  [ingestion.extra_parsers]
    pdf = ["zerox", "ocr"]

[ocr]
provider = "mistral"
model = "mistral-ocr-latest"

[orchestration]
provider = "simple"

[email]
provider = "console_mock" # `smtp`, `sendgrid`, and `mailersend` supported

[scheduler]
provider = "apscheduler"
